{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d9fa4d-9f1b-4d86-bb12-e3ee56e28027",
   "metadata": {},
   "source": [
    "# GPT2 math test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e1d64-8923-4a4c-8ff4-114aa0542c38",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This **Notebook** provides an interactive environment for exploring the capabilities of a fine-tuned GPT-2 model in solving math problems. You'll be able to load and compare a fine-tuned model against a base GPT-2 model, adjust various generation parameters on the fly, run predefined test cases, and even interactively pose your own math questions to the model. This setup allows for easy experimentation and observation of how different parameters affect the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e74652-1d48-485c-a5a7-8b83b7364def",
   "metadata": {},
   "source": [
    "## 2. Run It!\n",
    "This program consists of two parts: a comparison mode where the fine-tuned model's performance is gauged against a base GPT-2 model using a set of predefined test cases, and an interactive mode that allows you to directly input and receive answers for your own math problems using the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d68a8d-5bca-4464-b36e-b34795513354",
   "metadata": {},
   "source": [
    "### *Part A: Comparison Between Base GPT-2 Model and the Fine-tuned One* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498fa057-2e82-4090-bad9-b51a43737ff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T01:51:33.858415Z",
     "start_time": "2025-05-21T01:51:31.220288Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-21T02:17:30.477192Z",
     "iopub.status.busy": "2025-05-21T02:17:30.477192Z",
     "iopub.status.idle": "2025-05-21T02:17:30.481417Z",
     "shell.execute_reply": "2025-05-21T02:17:30.481417Z",
     "shell.execute_reply.started": "2025-05-21T02:17:30.477192Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b9f0ca3-e381-4d25-bd0e-4008f1b30e38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T01:51:33.858415Z",
     "start_time": "2025-05-21T01:51:31.220288Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-21T02:13:22.073125Z",
     "iopub.status.busy": "2025-05-21T02:13:22.073125Z",
     "iopub.status.idle": "2025-05-21T02:13:22.094530Z",
     "shell.execute_reply": "2025-05-21T02:13:22.093500Z",
     "shell.execute_reply.started": "2025-05-21T02:13:22.073125Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Global Variables for Models and Tokenizers ---\n",
    "# These will be populated after the \"Load Models\" cell is run\n",
    "fine_tuned_model = None\n",
    "fine_tuned_tokenizer = None\n",
    "base_model = None\n",
    "base_tokenizer = None\n",
    "current_device = None # To store the determined device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50474da9-84d4-456b-8085-608012e34462",
   "metadata": {},
   "source": [
    "The following section sets up **interactive widgets** that allow you to easily **adjust various parameters** for the GPT-2 models, such as model paths, generation length, temperature, and device settings, directly within the notebook interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afd27576-afba-4ad0-bb7e-c2cfae191302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T01:51:33.858415Z",
     "start_time": "2025-05-21T01:51:31.220288Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-21T02:17:33.755072Z",
     "iopub.status.busy": "2025-05-21T02:17:33.755072Z",
     "iopub.status.idle": "2025-05-21T02:17:33.776747Z",
     "shell.execute_reply": "2025-05-21T02:17:33.776747Z",
     "shell.execute_reply.started": "2025-05-21T02:17:33.755072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjust parameters below and then run the 'Load Models' cell:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fbaacab26e43bebd9e8d8bbef6fab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../models/fine_tuned_gpt2_math_2', description='Fine-tuned Model Path:', layout=Layout(width='80%'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d759fa3634ea40599ca29be61a2da577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='gpt2', description='Base Model Name:', layout=Layout(width='80%'), placeholder='e.g., gpt2, gpt2-m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770617a9104f47469f4ad3283c491d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../models', description='Cache Directory:', layout=Layout(width='80%'), placeholder='e.g., ../mode…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02833d4ae3c04cd0aed8db370ef8c92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=256, continuous_update=False, description='Max Generation Length:', max=512, min=50, step=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea9e22d50f5405c8931bf42872289e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.7, continuous_update=False, description='Temperature:', max=2.0, min=0.1, readout_format='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263ecc539e3e44e9b2fd7dca829cc77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.9, continuous_update=False, description='Top-p:', max=1.0, min=0.1, step=0.05)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a336a42d24704934b153d46e3d257c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('auto', 'cuda', 'cpu'), value='auto')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe8485a38594e1fa032515ec702a48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../res', description='Output Directory:', layout=Layout(width='80%'), placeholder='e.g., ../res')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Interactive Parameter Widgets ---\n",
    "# Define widgets for user input\n",
    "model_path_widget = widgets.Text(\n",
    "    value=\"../models/fine_tuned_gpt2_math_2\",\n",
    "    description=\"Fine-tuned Model Path:\",\n",
    "    placeholder=\"e.g., ../models/fine_tuned_gpt2_math_2\",\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "base_model_name_widget = widgets.Text(\n",
    "    value=\"gpt2\",\n",
    "    description=\"Base Model Name:\",\n",
    "    placeholder=\"e.g., gpt2, gpt2-medium\",\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "cache_dir_widget = widgets.Text(\n",
    "    value=\"../models\",\n",
    "    description=\"Cache Directory:\",\n",
    "    placeholder=\"e.g., ../models\",\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "max_length_widget = widgets.IntSlider(\n",
    "    value=256,\n",
    "    min=50,\n",
    "    max=512,\n",
    "    step=10,\n",
    "    description=\"Max Generation Length:\",\n",
    "    continuous_update=False, # Only update on release\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "temperature_widget = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.1,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description=\"Temperature:\",\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f'\n",
    ")\n",
    "\n",
    "top_p_widget = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description=\"Top-p:\",\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f'\n",
    ")\n",
    "\n",
    "device_widget = widgets.Dropdown(\n",
    "    options=[\"auto\", \"cuda\", \"cpu\"],\n",
    "    value=\"auto\",\n",
    "    description=\"Device:\",\n",
    ")\n",
    "\n",
    "output_dir_widget = widgets.Text(\n",
    "    value=\"../res\",\n",
    "    description=\"Output Directory:\",\n",
    "    placeholder=\"e.g., ../res\",\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Display all widgets\n",
    "print(\"Adjust parameters below and then run the 'Load Models' cell:\")\n",
    "display(\n",
    "    model_path_widget,\n",
    "    base_model_name_widget,\n",
    "    cache_dir_widget,\n",
    "    max_length_widget,\n",
    "    temperature_widget,\n",
    "    top_p_widget,\n",
    "    device_widget,\n",
    "    output_dir_widget\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f3ec7-a75f-4a50-9570-d480d91e8bc5",
   "metadata": {},
   "source": [
    "**Run the cell below to define the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680af104-c1da-41f5-ba81-458741b5a31d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T01:51:33.858415Z",
     "start_time": "2025-05-21T01:51:31.220288Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-21T02:19:50.034554Z",
     "iopub.status.busy": "2025-05-21T02:19:50.034554Z",
     "iopub.status.idle": "2025-05-21T02:19:50.046735Z",
     "shell.execute_reply": "2025-05-21T02:19:50.046735Z",
     "shell.execute_reply.started": "2025-05-21T02:19:50.034554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57961c48f7ef4fa58ced428c729afa14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Load Models', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec13f940f274891a61b984429638e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Load Models and Tokenizers (Interactive Function) ---\n",
    "# This cell will now use the values from the widgets\n",
    "def load_models_and_tokenizers(\n",
    "    model_path,\n",
    "    base_model_name,\n",
    "    cache_dir,\n",
    "    selected_device,\n",
    "    output_dir\n",
    "):\n",
    "    global fine_tuned_model, fine_tuned_tokenizer, base_model, base_tokenizer, current_device\n",
    "\n",
    "    clear_output(wait=True) # Clear previous output to show fresh loading status\n",
    "    print(f\"Loading models with parameters:\")\n",
    "    print(f\"  Fine-tuned Model Path: {model_path}\")\n",
    "    print(f\"  Base Model Name: {base_model_name}\")\n",
    "    print(f\"  Cache Directory: {cache_dir}\")\n",
    "    print(f\"  Selected Device: {selected_device}\")\n",
    "    print(f\"  Output Directory: {output_dir}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Determine the device\n",
    "    if selected_device == \"auto\":\n",
    "        current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        current_device = torch.device(selected_device)\n",
    "\n",
    "    print(f\"Using device: {current_device}\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Check if the fine-tuned model exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Fine-tuned model directory not found at '{model_path}'.\")\n",
    "        print(\"Please ensure the training program has been run and the model saved correctly.\")\n",
    "        fine_tuned_model = None\n",
    "        fine_tuned_tokenizer = None\n",
    "    else:\n",
    "        print(\"Loading fine-tuned model and tokenizer...\")\n",
    "        try:\n",
    "            fine_tuned_model = AutoModelForCausalLM.from_pretrained(model_path, cache_dir=cache_dir)\n",
    "            fine_tuned_tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)\n",
    "            # Ensure pad_token exists for consistent generation behavior\n",
    "            if fine_tuned_tokenizer.pad_token is None:\n",
    "                fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
    "                if fine_tuned_model.config.pad_token_id is None:\n",
    "                    fine_tuned_model.config.pad_token_id = fine_tuned_model.config.eos_token_id\n",
    "            fine_tuned_model.to(current_device)\n",
    "            fine_tuned_model.eval() # Set to evaluation mode\n",
    "            print(\"Fine-tuned model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fine-tuned model or tokenizer from '{model_path}': {e}\")\n",
    "            print(\"Please verify the path and ensure the directory contains valid model files.\")\n",
    "            fine_tuned_model = None\n",
    "            fine_tuned_tokenizer = None\n",
    "\n",
    "    print(f\"Loading base model '{base_model_name}' and tokenizer...\")\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir=cache_dir)\n",
    "        base_tokenizer = AutoTokenizer.from_pretrained(base_model_name, cache_dir=cache_dir)\n",
    "        # Ensure pad_token exists for consistent generation behavior\n",
    "        if base_tokenizer.pad_token is None:\n",
    "             base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "             if base_model.config.pad_token_id is None:\n",
    "                 base_model.config.pad_token_id = base_model.config.eos_token_id\n",
    "        base_model.to(current_device)\n",
    "        base_model.eval() # Set to evaluation mode\n",
    "        print(\"Base model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading base model or tokenizer '{base_model_name}': {e}\")\n",
    "        print(\"Please check the model name and your internet connection.\")\n",
    "        base_model = None\n",
    "        base_tokenizer = None\n",
    "\n",
    "# Use widgets.interactive to create an interactive UI for loading models\n",
    "# This will execute load_models_and_tokenizers whenever a widget value changes\n",
    "# We use a button to trigger the load explicitly, as loading models can be slow.\n",
    "load_button = widgets.Button(description=\"Load Models\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_load_button_clicked(b):\n",
    "    with output_area:\n",
    "        load_models_and_tokenizers(\n",
    "            model_path_widget.value,\n",
    "            base_model_name_widget.value,\n",
    "            cache_dir_widget.value,\n",
    "            device_widget.value,\n",
    "            output_dir_widget.value\n",
    "        )\n",
    "\n",
    "load_button.on_click(on_load_button_clicked)\n",
    "\n",
    "display(load_button, output_area)\n",
    "\n",
    "# --- Answer Generation Function ---\n",
    "def generate_answer(model, tokenizer, question):\n",
    "    \"\"\"Generates an answer to a math problem using the specified model and tokenizer.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"Error: Model or tokenizer not loaded. Please run the 'Load Models' cell first.\"\n",
    "\n",
    "    # Use the values from the widgets for generation parameters\n",
    "    max_length = max_length_widget.value\n",
    "    temperature = temperature_widget.value\n",
    "    top_p = top_p_widget.value\n",
    "\n",
    "    input_text = f\"Question: {question}\\nAnswer: \"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, return_attention_mask=True)\n",
    "    input_ids = inputs.input_ids.to(current_device)\n",
    "    attention_mask = inputs.attention_mask.to(current_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            early_stopping=False\n",
    "        )\n",
    "\n",
    "    num_prompt_tokens = input_ids.shape[1]\n",
    "    generated_ids = output_sequences[0][num_prompt_tokens:]\n",
    "    answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e7cbc1e-1a42-4df4-b5c4-af167347c6bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T01:51:33.858415Z",
     "start_time": "2025-05-21T01:51:31.220288Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-21T02:20:01.962245Z",
     "iopub.status.busy": "2025-05-21T02:20:01.962245Z",
     "iopub.status.idle": "2025-05-21T02:20:21.519709Z",
     "shell.execute_reply": "2025-05-21T02:20:21.519709Z",
     "shell.execute_reply.started": "2025-05-21T02:20:01.962245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running Tests and Saving Results =====\n",
      "\n",
      "Processing Problem 1...\n",
      "Generating answer with Base Model...\n",
      "Base Model Answer: It is the area of the rectangle, the width and height of the rectangle, the area of the rectangle, the width and height of the rectangle.\n",
      "Answer:  It is the area of the rectangle, the width and height of the rectangle, the area of the rectangle, the width and height of the rectangle.\n",
      "Answer:  It is the area of the rectangle, the width and height of the rectangle, the area of the rectangle, the width and height of the rectangle.\n",
      "Answer:  It is the area of the rectangle, the width and height of the rectangle, the area of the rectangle, the width and height of the rectangle.\n",
      "Answer:  It is the area of the rectangle, the width and height of the rectangle, the area of the rectangle, the width and height of the rectangle.\n",
      "Answer:  It is the area of the rectangle, the width and height of the rectangle, the area of the rectangle, the width and height of the rectangle.\n",
      "Answer:  It is the area of the rectangle, the width and height of the rectangle, the area\n",
      "Generating answer with Fine-tuned Model...\n",
      "Fine-tuned Model Answer: The area of a rectangle is calculated by adding the lengths of its sides, the width, and the height together.\n",
      "\n",
      "The area of a rectangle is calculated by adding the lengths of its sides, the width, and the height together.\n",
      "\n",
      "Area = Length × Width × Height\n",
      "Area = 8 meters × 5 meters\n",
      "Area = 32 meters\n",
      "\n",
      "So, the area of the rectangle is 32 meters.\n",
      "\n",
      "Now, to find the area of the rectangle, we use the formula:\n",
      "\n",
      "Area = Length × Width × Height\n",
      "\n",
      "Area = 32 meters × 5 meters\n",
      "Area = 168 meters²\n",
      "\n",
      "Therefore, the area of the rectangle is 168 meters².\n",
      "\n",
      "To find the area of the rectangle, we divide the area by the width:\n",
      "\n",
      "Area = 168 meters² / 5 meters\n",
      "Area = 16.67 meters²\n",
      "\n",
      "Therefore, the area of the rectangle is 16.67 meters².\n",
      "\n",
      "Now, to find the area of the rectangle, we divide the area by the width:\n",
      "\n",
      "Area = 16.67 meters² / 5 meters\n",
      "Area = 3.66 meters\n",
      "\n",
      "Processing Problem 2...\n",
      "Generating answer with Base Model...\n",
      "Base Model Answer: It's not 100%, but I would guess that there are a lot of apples left.\n",
      "What is the total number of apples left?\n",
      "Answer:  The total number of apples left is a bit over 100.\n",
      "What is the total number of apples left?\n",
      "Answer:  I would guess that there are a lot of apples left.\n",
      "What is the total number of apples left?\n",
      "Answer:  I would guess that there are a lot of apples left.\n",
      "What is the total number of apples left?\n",
      "Answer:  I would guess that there are a lot of apples left.\n",
      "What is the total number of apples left?\n",
      "Answer:  I would guess that there are a lot of apples left.\n",
      "What is the total number of apples left?\n",
      "Answer:  I would guess that there are a lot of apples left.\n",
      "What is the total number of apples left?\n",
      "Answer:  I would guess that there are a lot of apples left.\n",
      "What is the total number of apples left?\n",
      "Answer:  I would guess\n",
      "Generating answer with Fine-tuned Model...\n",
      "Fine-tuned Model Answer: Ming originally has 12 apples. After giving 3 to Hong, he gives 2 to Gang, so he has 12 - 3 = 12 - 2 = 9 apples left.\n",
      "\n",
      "After giving to Hong, he has 9 - 2 = 6 apples left.\n",
      "\n",
      "Since Ming has 12 apples left, he has 6 - 12 = 4 apples left to give to his friends.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "To find out how many apples Ming has left, we subtract the number of apples he gave away from the number he has left.\n",
      "\n",
      "Ming has 4 apples left - 6 apples left = 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Therefore, Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples left.\n",
      "\n",
      "Ming has 4 apples.\n",
      "\n",
      "Processing Problem 3...\n",
      "Generating answer with Base Model...\n",
      "Base Model Answer: The circumference of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\n",
      "Answer:  The circumference of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\n",
      "Answer:  The circumference of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\n",
      "Answer:  The circumference of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\n",
      "Answer:  The circumference of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\n",
      "Answer:  The circumference of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\n",
      "Answer:  The circumference of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\n",
      "Answer:  The circumference of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\n",
      "Answer:  The circumference\n",
      "Generating answer with Fine-tuned Model...\n",
      "Fine-tuned Model Answer: The circumference of a circle (C) is given by the formula:\n",
      "C = π * r^2\n",
      "where r is the radius of the circle.\n",
      "\n",
      "Given that the radius is 6 centimeters, we can set up the equation:\n",
      "6 cm = π * (3.14)^2\n",
      "\n",
      "Now, let's solve for r:\n",
      "6 cm = (3.14)^2 - 6 cm\n",
      "6 cm = 0.6 * 0.6\n",
      "6 cm = 0.3 * 0.3\n",
      "6 cm = 0.6 cm\n",
      "r = 6 cm / 0.6 cm\n",
      "r = 0.6 cm / 1.5 cm\n",
      "r = 0.4 cm\n",
      "\n",
      "So, the circumference of the circle is 0.4 cm.\n",
      "\n",
      "Now that we have the circumference, we can find the circumference of the circle by multiplying the radius by the circumference of the circle.\n",
      "\n",
      "Circumference of the circle = (Circumference * Radius) / Circumference of the circle\n",
      "Circumference of the circle = 0.4 cm / 0\n",
      "\n",
      "Processing Problem 4...\n",
      "Generating answer with Base Model...\n",
      "Base Model Answer: It depends on the number of trains.\n",
      "If you need to travel at a speed of 72 kilometers per hour, then it depends on the number of trains.\n",
      "The average speed of a train is about 12.5 kilometers per hour. The average speed of a train is about 16 kilometers per hour.\n",
      "The average speed of a train is about 2.5 kilometers per hour.\n",
      "In order to travel at a speed of 2.5 kilometers per hour, you must have a speed of at least 8 kilometers per hour.\n",
      "The average speed of a train is about 20 kilometers per hour.\n",
      "You need a speed of at least 18 kilometers per hour.\n",
      "If you need to travel at a speed of 18 kilometers per hour, you need a speed of at least 24 kilometers per hour.\n",
      "In order to travel at a speed of 24 kilometers per hour, you need a speed of at least 30 kilometers per hour.\n",
      "If you need to travel at a speed of 30 kilometers per hour, you need a speed of at least 40 kilometers per hour.\n",
      "You need a speed of at least 50 kilometers per hour\n",
      "Generating answer with Fine-tuned Model...\n",
      "Fine-tuned Model Answer: To find out how many kilometers it can travel in 2.5 hours, we first need to calculate the total distance the train travels. \n",
      "\n",
      "The distance to be traveled in 2.5 hours is:\n",
      "72 kilometers (total distance) * 2.5 hours/train = 375 kilometers (total distance)\n",
      "\n",
      "Now, we know the total distance to be traveled is 375 kilometers. To find out how many kilometers the train can travel in 2.5 hours, we divide the total distance by the time:\n",
      "\n",
      "375 kilometers ÷ 2.5 hours/train = 40 kilometers/hour\n",
      "\n",
      "Therefore, the train can travel 40 kilometers in 2.5 hours. To find out how many kilometers it can travel in 2.5 hours, we divide the total distance by the time:\n",
      "\n",
      "40 kilometers / 2.5 hours/train = 1.5 kilometers\n",
      "\n",
      "Therefore, the train can travel 1.5 kilometers in 2.5 hours.\n",
      "\n",
      "To convert kilometers to kilometers, we multiply by 100:\n",
      "\n",
      "1.5 kilometers ÷ 100 kilometers/km = 2.5 kilometers\n",
      "\n",
      "Processing Problem 5...\n",
      "Generating answer with Base Model...\n",
      "Base Model Answer: If a store sells all of the items in the batch, it will sell a lot of the items in the batch. However, if there is a shortage, the store will sell only the items that were previously in the batch.\n",
      "Question: How many items are there in the inventory?\n",
      "Answer:  In most cases, the inventory will be there.\n",
      "Question: What are the limits on what can be stored?\n",
      "Answer:  Most stores will sell more than they sell in a single day. In most cases, you will only have one item in the inventory at a time.\n",
      "Question: Is there a limit to how many items can be stored?\n",
      "Answer:  The limit is the number of items in the inventory at the time of sale. For example, if you sell 10,000 items in a single day, you can only have one item in the inventory.\n",
      "Question: What is the limit on the quantity of items that can be stored\n",
      "Generating answer with Fine-tuned Model...\n",
      "Fine-tuned Model Answer: If 120 items were sold on the first day, then the total number of items sold on the first day is:\n",
      "   120 * 1/3 = 240\n",
      "\n",
      "On the second day, 40% of the remaining were sold, so the remaining percentage is:\n",
      "   0.40 * 240 = 40%\n",
      "\n",
      "So, the original number of items in the batch is 240.\n",
      "\n",
      "Since 120 items are left, the remaining items after the second day are:\n",
      "   240 - 120 = 120 - 40\n",
      "\n",
      "This means that the original quantity of goods in the batch was 120 items.\n",
      "\n",
      "Now, to find the initial total number of items in the batch, we subtract the original number of items from the original number of items:\n",
      "   Initial quantity of goods = Original quantity - Original quantity\n",
      "   Initial quantity of goods = 240 - 120\n",
      "   Initial quantity of goods = 90\n",
      "\n",
      "Therefore, there were initially 90 items in the\n",
      "\n",
      "===== Testing Complete. Results saved to ../res\\math_test_results_20250521_102001.txt =====\n"
     ]
    }
   ],
   "source": [
    "# --- Run Test Cases ---\n",
    "print(\"\\n===== Running Tests and Saving Results =====\")\n",
    "\n",
    "test_problems = [\n",
    "    \"If the length of a rectangle is 8 meters and the width is 5 meters, what is its area?\",\n",
    "    \"Ming has 12 apples. He gives 3 to Hong and 2 to Gang. How many apples does Ming have left?\",\n",
    "    \"The radius of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\",\n",
    "    \"A train travels at a speed of 72 kilometers per hour. How many kilometers can it travel in 2.5 hours?\",\n",
    "    \"A shop has a batch of goods. On the first day, 1/3 of the total was sold. On the second day, 40% of the remainder was sold. If 120 items are left, how many items were there originally in this batch?\"\n",
    "]\n",
    "\n",
    "# Define output file path with a timestamp\n",
    "output_dir = output_dir_widget.value # Get the output directory from the widget\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"math_test_results_{timestamp}.txt\"\n",
    "output_filepath = os.path.join(output_dir, output_filename)\n",
    "\n",
    "with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Math Problem Test Results\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Fine-tuned Model: {model_path_widget.value}\\n\")\n",
    "    f.write(f\"Base Model: {base_model_name_widget.value}\\n\")\n",
    "    f.write(f\"Device: {current_device}\\n\")\n",
    "    f.write(f\"Max Length: {max_length_widget.value}\\n\")\n",
    "    f.write(f\"Temperature: {temperature_widget.value}\\n\")\n",
    "    f.write(f\"Top-p: {top_p_widget.value}\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    for i, problem in enumerate(test_problems, 1):\n",
    "        print(f\"\\nProcessing Problem {i}...\")\n",
    "        f.write(f\"===== Problem {i} =====\\n\")\n",
    "        f.write(f\"Question: {problem}\\n\\n\")\n",
    "\n",
    "        # Generate answer from base model\n",
    "        if base_model and base_tokenizer:\n",
    "            print(\"Generating answer with Base Model...\")\n",
    "            base_answer = generate_answer(base_model, base_tokenizer, problem)\n",
    "            f.write(f\"Base Model Answer: {base_answer}\\n\\n\")\n",
    "            print(f\"Base Model Answer: {base_answer}\")\n",
    "        else:\n",
    "            f.write(\"Base Model not loaded, skipping generation.\\n\\n\")\n",
    "            print(\"Base Model not loaded, skipping generation.\")\n",
    "\n",
    "        # Generate answer from fine-tuned model\n",
    "        if fine_tuned_model and fine_tuned_tokenizer:\n",
    "            print(\"Generating answer with Fine-tuned Model...\")\n",
    "            fine_tuned_answer = generate_answer(fine_tuned_model, fine_tuned_tokenizer, problem)\n",
    "            f.write(f\"Fine-tuned Model Answer: {fine_tuned_answer}\\n\")\n",
    "            print(f\"Fine-tuned Model Answer: {fine_tuned_answer}\")\n",
    "        else:\n",
    "            f.write(\"Fine-tuned Model not loaded, skipping generation.\\n\")\n",
    "            print(\"Fine-tuned Model not loaded, skipping generation.\")\n",
    "\n",
    "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "print(f\"\\n===== Testing Complete. Results saved to {output_filepath} =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f7ede-3815-4c5a-ace6-fe62d5e916f8",
   "metadata": {},
   "source": [
    "### *Part B: Interactive Math Test for Fine-tune GTP-2 Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8e9a0e2-3527-4fd8-8099-e3d429cc44ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T01:51:33.858415Z",
     "start_time": "2025-05-21T01:51:31.220288Z"
    },
    "execution": {
     "iopub.execute_input": "2025-05-21T02:20:28.784901Z",
     "iopub.status.busy": "2025-05-21T02:20:28.784901Z",
     "iopub.status.idle": "2025-05-21T02:20:28.813263Z",
     "shell.execute_reply": "2025-05-21T02:20:28.813263Z",
     "shell.execute_reply.started": "2025-05-21T02:20:28.784901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Interactive Mode (using Fine-tuned Model) =====\n",
      "Enter math problems below. Type 'q' or 'quit' to exit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930220a0b0ad4dbaa7b478a71fa07018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Problem:', layout=Layout(height='100px', width='80%'), placeholder='Enter a ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8727b74deeb24e959e83932464fb6376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Answer', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0f77149c094b429c554125f37e3b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache cleared.\n"
     ]
    }
   ],
   "source": [
    "# --- Interactive Mode ---\n",
    "print(\"\\n===== Interactive Mode (using Fine-tuned Model) =====\")\n",
    "print(\"Enter math problems below. Type 'q' or 'quit' to exit.\")\n",
    "\n",
    "# Create a text area for user input in interactive mode\n",
    "interactive_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Enter a math problem here...\",\n",
    "    description=\"Problem:\",\n",
    "    layout=widgets.Layout(width='80%', height='100px')\n",
    ")\n",
    "\n",
    "# Create a button to generate answer\n",
    "generate_button = widgets.Button(description=\"Generate Answer\")\n",
    "\n",
    "# Create an output area for the model's answer\n",
    "interactive_output_area = widgets.Output()\n",
    "\n",
    "def on_generate_button_clicked(b):\n",
    "    with interactive_output_area:\n",
    "        clear_output(wait=True)\n",
    "        user_input = interactive_input.value.strip()\n",
    "        if not user_input:\n",
    "            print(\"Input cannot be empty. Please enter a problem.\")\n",
    "            return\n",
    "\n",
    "        if fine_tuned_model and fine_tuned_tokenizer:\n",
    "            print(\"Generating answer...\")\n",
    "            answer = generate_answer(fine_tuned_model, fine_tuned_tokenizer, user_input)\n",
    "            print(f\"Model Answer: {answer}\")\n",
    "        else:\n",
    "            print(\"Fine-tuned model not available for interactive mode. Please load models first.\")\n",
    "\n",
    "generate_button.on_click(on_generate_button_clicked)\n",
    "\n",
    "# Display interactive widgets\n",
    "display(interactive_input, generate_button, interactive_output_area)\n",
    "\n",
    "# Optional: Clear CUDA cache if using GPU, to free up memory\n",
    "if current_device and current_device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6f486cc9cf76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
