{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "86d9fa4d-9f1b-4d86-bb12-e3ee56e28027",
      "metadata": {
        "id": "86d9fa4d-9f1b-4d86-bb12-e3ee56e28027"
      },
      "source": [
        "# GPT2 math test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "545e1d64-8923-4a4c-8ff4-114aa0542c38",
      "metadata": {
        "id": "545e1d64-8923-4a4c-8ff4-114aa0542c38"
      },
      "source": [
        "## 1. Introduction\n",
        "This **Notebook** provides an interactive environment for exploring the capabilities of a fine-tuned GPT-2 model in solving math problems. You'll be able to load and compare a fine-tuned model against a base GPT-2 model, adjust various generation parameters on the fly, run predefined test cases, and even interactively pose your own math questions to the model. This setup allows for easy experimentation and observation of how different parameters affect the model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e74652-1d48-485c-a5a7-8b83b7364def",
      "metadata": {
        "id": "87e74652-1d48-485c-a5a7-8b83b7364def"
      },
      "source": [
        "## 2. Run It!\n",
        "This program consists of two parts: a comparison mode where the fine-tuned model's performance is gauged against a base GPT-2 model using a set of predefined test cases, and an interactive mode that allows you to directly input and receive answers for your own math problems using the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# 1. 挂载 Google Drive\n",
        "# ---------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWXlJn6JWNvE",
        "outputId": "578c3c98-3967-4356-b282-07ecb9458222"
      },
      "id": "KWXlJn6JWNvE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d68a8d-5bca-4464-b36e-b34795513354",
      "metadata": {
        "id": "c9d68a8d-5bca-4464-b36e-b34795513354"
      },
      "source": [
        "### *Part A: Comparison Between Base GPT-2 Model and the Fine-tuned One*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "498fa057-2e82-4090-bad9-b51a43737ff2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-21T01:51:33.858415Z",
          "start_time": "2025-05-21T01:51:31.220288Z"
        },
        "execution": {
          "iopub.execute_input": "2025-05-21T16:30:46.482877Z",
          "iopub.status.busy": "2025-05-21T16:30:46.482877Z",
          "iopub.status.idle": "2025-05-21T16:30:51.277212Z",
          "shell.execute_reply": "2025-05-21T16:30:51.277212Z",
          "shell.execute_reply.started": "2025-05-21T16:30:46.482877Z"
        },
        "id": "498fa057-2e82-4090-bad9-b51a43737ff2"
      },
      "outputs": [],
      "source": [
        "# --- Import Libraries ---\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datetime import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9f0ca3-e381-4d25-bd0e-4008f1b30e38",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-21T16:07:04.907321Z",
          "start_time": "2025-05-21T16:07:04.902291Z"
        },
        "execution": {
          "iopub.execute_input": "2025-05-21T16:30:57.689726Z",
          "iopub.status.busy": "2025-05-21T16:30:57.688725Z",
          "iopub.status.idle": "2025-05-21T16:30:57.692245Z",
          "shell.execute_reply": "2025-05-21T16:30:57.692245Z",
          "shell.execute_reply.started": "2025-05-21T16:30:57.689726Z"
        },
        "id": "8b9f0ca3-e381-4d25-bd0e-4008f1b30e38"
      },
      "outputs": [],
      "source": [
        "# --- Global Variables for Models and Tokenizers ---\n",
        "# These will be populated after the \"Load Models\" cell is run\n",
        "fine_tuned_model = None\n",
        "fine_tuned_tokenizer = None\n",
        "base_model = None\n",
        "base_tokenizer = None\n",
        "current_device = None # To store the determined device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50474da9-84d4-456b-8085-608012e34462",
      "metadata": {
        "id": "50474da9-84d4-456b-8085-608012e34462"
      },
      "source": [
        "The following section sets up **interactive widgets** that allow you to easily **adjust various parameters** for the GPT-2 models, such as model paths, generation length, temperature, and device settings, directly within the notebook interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd27576-afba-4ad0-bb7e-c2cfae191302",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-21T01:51:33.858415Z",
          "start_time": "2025-05-21T01:51:31.220288Z"
        },
        "execution": {
          "iopub.execute_input": "2025-05-21T16:31:00.108620Z",
          "iopub.status.busy": "2025-05-21T16:31:00.108620Z",
          "iopub.status.idle": "2025-05-21T16:31:00.129289Z",
          "shell.execute_reply": "2025-05-21T16:31:00.129289Z",
          "shell.execute_reply.started": "2025-05-21T16:31:00.108620Z"
        },
        "colab": {
          "referenced_widgets": [
            "e1a3f578e2a54ac5a20e2d22a28acd5c",
            "45ed0074dee24dcb975baf618cfb3a67",
            "720bb4739132444fae6f580216412784",
            "d178bcd0d2f540e3a07dba0c5caab2e0",
            "bbd33c2f68e74ecb9d4e4cf77ce2e8fd",
            "d8912e932b3e40c98bba82fdabb30929",
            "80aef4cf6dac43dc8bb3cde371f6cd5a",
            "9bb7ff74487b45959ff831d3d1b33660"
          ]
        },
        "id": "afd27576-afba-4ad0-bb7e-c2cfae191302",
        "outputId": "c1c1d800-871e-488a-c36f-6b22e828a1b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjust parameters below and then run the 'Load Models' cell:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1a3f578e2a54ac5a20e2d22a28acd5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='../models/fine_tuned_gpt2_math_2', description='Fine-tuned Model Path:', layout=Layout(width='80%'…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45ed0074dee24dcb975baf618cfb3a67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='gpt2', description='Base Model Name:', layout=Layout(width='80%'), placeholder='e.g., gpt2, gpt2-m…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "720bb4739132444fae6f580216412784",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='../models', description='Cache Directory:', layout=Layout(width='80%'), placeholder='e.g., ../mode…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d178bcd0d2f540e3a07dba0c5caab2e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "IntSlider(value=256, continuous_update=False, description='Max Generation Length:', max=512, min=50, step=10)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbd33c2f68e74ecb9d4e4cf77ce2e8fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatSlider(value=0.7, continuous_update=False, description='Temperature:', max=2.0, min=0.1, readout_format='…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8912e932b3e40c98bba82fdabb30929",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatSlider(value=0.9, continuous_update=False, description='Top-p:', max=1.0, min=0.1, step=0.05)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80aef4cf6dac43dc8bb3cde371f6cd5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(description='Device:', options=('auto', 'cuda', 'cpu'), value='auto')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bb7ff74487b45959ff831d3d1b33660",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='../res', description='Output Directory:', layout=Layout(width='80%'), placeholder='e.g., ../res')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Interactive Parameter Widgets ---\n",
        "# Define widgets for user input\n",
        "model_path_widget = widgets.Text(\n",
        "    value=\"../models/fine_tuned_gpt2_math_2\",\n",
        "    description=\"Fine-tuned Model Path:\",\n",
        "    placeholder=\"e.g., ../models/fine_tuned_gpt2_math_2\",\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "base_model_name_widget = widgets.Text(\n",
        "    value=\"gpt2\",\n",
        "    description=\"Base Model Name:\",\n",
        "    placeholder=\"e.g., gpt2, gpt2-medium\",\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "cache_dir_widget = widgets.Text(\n",
        "    value=\"../models\",\n",
        "    description=\"Cache Directory:\",\n",
        "    placeholder=\"e.g., ../models\",\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "max_length_widget = widgets.IntSlider(\n",
        "    value=256,\n",
        "    min=50,\n",
        "    max=512,\n",
        "    step=10,\n",
        "    description=\"Max Generation Length:\",\n",
        "    continuous_update=False, # Only update on release\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='d'\n",
        ")\n",
        "\n",
        "temperature_widget = widgets.FloatSlider(\n",
        "    value=0.7,\n",
        "    min=0.1,\n",
        "    max=2.0,\n",
        "    step=0.1,\n",
        "    description=\"Temperature:\",\n",
        "    continuous_update=False,\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.1f'\n",
        ")\n",
        "\n",
        "top_p_widget = widgets.FloatSlider(\n",
        "    value=0.9,\n",
        "    min=0.1,\n",
        "    max=1.0,\n",
        "    step=0.05,\n",
        "    description=\"Top-p:\",\n",
        "    continuous_update=False,\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.2f'\n",
        ")\n",
        "\n",
        "device_widget = widgets.Dropdown(\n",
        "    options=[\"auto\", \"cuda\", \"cpu\"],\n",
        "    value=\"auto\",\n",
        "    description=\"Device:\",\n",
        ")\n",
        "\n",
        "output_dir_widget = widgets.Text(\n",
        "    value=\"../res\",\n",
        "    description=\"Output Directory:\",\n",
        "    placeholder=\"e.g., ../res\",\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "# Display all widgets\n",
        "print(\"Adjust parameters below and then run the 'Load Models' cell:\")\n",
        "display(\n",
        "    model_path_widget,\n",
        "    base_model_name_widget,\n",
        "    cache_dir_widget,\n",
        "    max_length_widget,\n",
        "    temperature_widget,\n",
        "    top_p_widget,\n",
        "    device_widget,\n",
        "    output_dir_widget\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c1f3ec7-a75f-4a50-9570-d480d91e8bc5",
      "metadata": {
        "id": "8c1f3ec7-a75f-4a50-9570-d480d91e8bc5"
      },
      "source": [
        "**Run the cell below to define the function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680af104-c1da-41f5-ba81-458741b5a31d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-21T01:51:33.858415Z",
          "start_time": "2025-05-21T01:51:31.220288Z"
        },
        "execution": {
          "iopub.execute_input": "2025-05-21T16:31:37.563091Z",
          "iopub.status.busy": "2025-05-21T16:31:37.563091Z",
          "iopub.status.idle": "2025-05-21T16:31:37.575571Z",
          "shell.execute_reply": "2025-05-21T16:31:37.575571Z",
          "shell.execute_reply.started": "2025-05-21T16:31:37.563091Z"
        },
        "colab": {
          "referenced_widgets": [
            "9c5e7b16201447c5a1d081446c281410",
            "31ecdfac69474933a06529d031956123"
          ]
        },
        "id": "680af104-c1da-41f5-ba81-458741b5a31d",
        "outputId": "b1822a0b-5293-4136-84ad-4bfad06d6ef5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c5e7b16201447c5a1d081446c281410",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(description='Load Models', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31ecdfac69474933a06529d031956123",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Load Models and Tokenizers (Interactive Function) ---\n",
        "# This cell will now use the values from the widgets\n",
        "def load_models_and_tokenizers(\n",
        "    model_path,\n",
        "    base_model_name,\n",
        "    cache_dir,\n",
        "    selected_device,\n",
        "    output_dir\n",
        "):\n",
        "    global fine_tuned_model, fine_tuned_tokenizer, base_model, base_tokenizer, current_device\n",
        "\n",
        "    clear_output(wait=True) # Clear previous output to show fresh loading status\n",
        "    print(f\"Loading models with parameters:\")\n",
        "    print(f\"  Fine-tuned Model Path: {model_path}\")\n",
        "    print(f\"  Base Model Name: {base_model_name}\")\n",
        "    print(f\"  Cache Directory: {cache_dir}\")\n",
        "    print(f\"  Selected Device: {selected_device}\")\n",
        "    print(f\"  Output Directory: {output_dir}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Determine the device\n",
        "    if selected_device == \"auto\":\n",
        "        current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    else:\n",
        "        current_device = torch.device(selected_device)\n",
        "\n",
        "    print(f\"Using device: {current_device}\")\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Check if the fine-tuned model exists\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Error: Fine-tuned model directory not found at '{model_path}'.\")\n",
        "        print(\"Please ensure the training program has been run and the model saved correctly.\")\n",
        "        fine_tuned_model = None\n",
        "        fine_tuned_tokenizer = None\n",
        "    else:\n",
        "        print(\"Loading fine-tuned model and tokenizer...\")\n",
        "        try:\n",
        "            fine_tuned_model = AutoModelForCausalLM.from_pretrained(model_path, cache_dir=cache_dir)\n",
        "            fine_tuned_tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)\n",
        "            # Ensure pad_token exists for consistent generation behavior\n",
        "            if fine_tuned_tokenizer.pad_token is None:\n",
        "                fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
        "                if fine_tuned_model.config.pad_token_id is None:\n",
        "                    fine_tuned_model.config.pad_token_id = fine_tuned_model.config.eos_token_id\n",
        "            fine_tuned_model.to(current_device)\n",
        "            fine_tuned_model.eval() # Set to evaluation mode\n",
        "            print(\"Fine-tuned model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading fine-tuned model or tokenizer from '{model_path}': {e}\")\n",
        "            print(\"Please verify the path and ensure the directory contains valid model files.\")\n",
        "            fine_tuned_model = None\n",
        "            fine_tuned_tokenizer = None\n",
        "\n",
        "    print(f\"Loading base model '{base_model_name}' and tokenizer...\")\n",
        "    try:\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir=cache_dir)\n",
        "        base_tokenizer = AutoTokenizer.from_pretrained(base_model_name, cache_dir=cache_dir)\n",
        "        # Ensure pad_token exists for consistent generation behavior\n",
        "        if base_tokenizer.pad_token is None:\n",
        "             base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "             if base_model.config.pad_token_id is None:\n",
        "                 base_model.config.pad_token_id = base_model.config.eos_token_id\n",
        "        base_model.to(current_device)\n",
        "        base_model.eval() # Set to evaluation mode\n",
        "        print(\"Base model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading base model or tokenizer '{base_model_name}': {e}\")\n",
        "        print(\"Please check the model name and your internet connection.\")\n",
        "        base_model = None\n",
        "        base_tokenizer = None\n",
        "\n",
        "# Use widgets.interactive to create an interactive UI for loading models\n",
        "# This will execute load_models_and_tokenizers whenever a widget value changes\n",
        "# We use a button to trigger the load explicitly, as loading models can be slow.\n",
        "load_button = widgets.Button(description=\"Load Models\")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_load_button_clicked(b):\n",
        "    with output_area:\n",
        "        load_models_and_tokenizers(\n",
        "            model_path_widget.value,\n",
        "            base_model_name_widget.value,\n",
        "            cache_dir_widget.value,\n",
        "            device_widget.value,\n",
        "            output_dir_widget.value\n",
        "        )\n",
        "\n",
        "load_button.on_click(on_load_button_clicked)\n",
        "\n",
        "display(load_button, output_area)\n",
        "\n",
        "# --- Answer Generation Function ---\n",
        "def generate_answer(model, tokenizer, question):\n",
        "    \"\"\"Generates an answer to a math problem using the specified model and tokenizer.\"\"\"\n",
        "    if model is None or tokenizer is None:\n",
        "        return \"Error: Model or tokenizer not loaded. Please run the 'Load Models' cell first.\"\n",
        "\n",
        "    # Use the values from the widgets for generation parameters\n",
        "    max_length = max_length_widget.value\n",
        "    temperature = temperature_widget.value\n",
        "    top_p = top_p_widget.value\n",
        "\n",
        "    input_text = f\"Question: {question}\\nAnswer: \"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, return_attention_mask=True)\n",
        "    input_ids = inputs.input_ids.to(current_device)\n",
        "    attention_mask = inputs.attention_mask.to(current_device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_sequences = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            early_stopping=False\n",
        "        )\n",
        "\n",
        "    num_prompt_tokens = input_ids.shape[1]\n",
        "    generated_ids = output_sequences[0][num_prompt_tokens:]\n",
        "    answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e7cbc1e-1a42-4df4-b5c4-af167347c6bf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-21T01:51:33.858415Z",
          "start_time": "2025-05-21T01:51:31.220288Z"
        },
        "execution": {
          "iopub.execute_input": "2025-05-21T16:31:42.475362Z",
          "iopub.status.busy": "2025-05-21T16:31:42.474362Z",
          "iopub.status.idle": "2025-05-21T16:32:00.322968Z",
          "shell.execute_reply": "2025-05-21T16:32:00.322968Z",
          "shell.execute_reply.started": "2025-05-21T16:31:42.475362Z"
        },
        "id": "8e7cbc1e-1a42-4df4-b5c4-af167347c6bf",
        "outputId": "0f212540-5111-47b9-e4c3-da561615e13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Running Tests and Saving Results =====\n",
            "\n",
            "Processing Problem 1...\n",
            "Generating answer with Base Model...\n",
            "Base Model Answer: In the above diagram, the width of the rectangle is 8 meters and the height is 5 meters. The rectangle has a width of 7 meters and a height of 3 meters.  Therefore, a rectangle of length 7 meters and width 3 meters, as shown in the figure, is covered by the size of a rectangle of 7 meters and width 3 meters.  The rectangle is not covered by the size of a rectangle of 6 meters and width 6 meters.  Therefore, a rectangle of length 6 meters and width 6 meters is covered by the size of a rectangle of 6 meters and width 6 meters.  The rectangle is not covered by the size of a rectangle of 5 meters and width 5 meters.  Therefore, a rectangle of length 5 meters and width 5 meters is covered by the size of a rectangle of 4 meters and width 4 meters.  Therefore, a rectangle of length 4 meters and width 4 meters is covered by the size of a rectangle of 3 meters and width 3 meters.  Therefore, a rectangle of length 2 meters and width 2 meters is covered by the size of a rectangle of 1\n",
            "Generating answer with Fine-tuned Model...\n",
            "Fine-tuned Model Answer: If the length of the rectangle is 8 meters and the width is 5 meters, then the area of the rectangle is:\n",
            "\n",
            "Area = Width × Height\n",
            "Area = 8 meters × 5 meters\n",
            "Area = 300 square meters\n",
            "\n",
            "So, the area of the rectangle is 300 square meters.\n",
            "\n",
            "Now, if the length of the rectangle is 8 meters and the width is 5 meters, then the area of the rectangle is:\n",
            "\n",
            "Area = Length × Width\n",
            "Area = 8 meters × 5 meters\n",
            "Area = 300 square meters\n",
            "\n",
            "Therefore, the area of the rectangle is 300 square meters.\n",
            "\n",
            "The area of a rectangle is calculated by dividing the area of the rectangle by its length and by multiplying by 100. The area of a rectangle is calculated by multiplying by its side length and by dividing by 100. The area of a rectangle is calculated by multiplying by its perimeter and multiplying by 100. The area of a rectangle is calculated by multiplying by its perimeter and multiplying by 100.\n",
            "\n",
            "So, the area of the rectangle is:\n",
            "\n",
            "Area = Length × Width\n",
            "Area = 300 square meters × 100\n",
            "Area =\n",
            "\n",
            "Processing Problem 2...\n",
            "Generating answer with Base Model...\n",
            "Base Model Answer: Hong and Gang give 6 to Ming and 1 to Ming.\n",
            "Question: Ming has 12 apples. He gives 3 to Hong and 2 to Hong. How many apples does Ming have left?\n",
            "Answer:  Hong and Gang give 6 to Ming and 1 to Ming.\n",
            "Question: Ming has 12 apples. He gives 3 to Hong and 2 to Hong. How many apples does Ming have left?\n",
            "Answer:  Hong and Gang give 6 to Ming and 1 to Ming.\n",
            "Question: Ming has 12 apples. He gives 3 to Hong and 2 to Hong. How many apples does Ming have left?\n",
            "Answer:  Hong and Gang give 6 to Ming and 1 to Ming.\n",
            "Question: Ming has 12 apples. He gives 3 to Hong and 2 to Hong. How many apples does Ming have left?\n",
            "Answer:  Hong and Gang give 6 to Ming and 1 to Ming.\n",
            "Question: Ming has 12 apples. He gives 3 to Hong and 2 to Hong. How many apples does Ming have left?\n",
            "Answer:  Hong and Gang give 6 to Ming and 1 to Ming\n",
            "Generating answer with Fine-tuned Model...\n",
            "Fine-tuned Model Answer: Ming originally had 12 apples. After giving away 3 apples, he has 12 - 3 = 9 apples left.\n",
            "\n",
            "He gave away 2 apples, so he has 9 - 2 = 9 apples left.\n",
            "\n",
            "Since he gave away 3 apples, the number of apples he has left is 9 - 3 = 5 apples.\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "To find out how many apples Ming has left, we subtract the number of apples he gave away from the number he has left:\n",
            "\n",
            "9 (left) - 5 (left) = 5 apples\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "So, Ming has 5 apples left.\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "Ming has 5 apples left.\n",
            "\n",
            "Ming has 5 apples.\n",
            "\n",
            "Ming has 5 apples.\n",
            "\n",
            "Ming has 5 apples.\n",
            "\n",
            "Ming has\n",
            "\n",
            "Processing Problem 3...\n",
            "Generating answer with Base Model...\n",
            "Base Model Answer: The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference of a circle is 6 centimeters. What is its circumference?  The circumference\n",
            "Generating answer with Fine-tuned Model...\n",
            "Fine-tuned Model Answer: If the radius of the circle is 6 centimeters, we can find the circumference by taking the square root of the circumference.\n",
            "\n",
            "Circumference = π * (pi * 6)\n",
            "Circumference = π * 6 * 3.14\n",
            "Circumference = 18.14 cm\n",
            "\n",
            "So, the circumference of the circle is 18.14 cm.\n",
            "\n",
            "To find the circumference, we can use the formula:\n",
            "\n",
            "Circumference = π * (pi * 6) / (18.14 cm)\n",
            "\n",
            "To simplify the fraction, we can divide both the numerator and the denominator by 6 and multiply by the greatest common divisor.\n",
            "\n",
            "Circumference = (18.14 cm / 6) * 6 * 3.14\n",
            "Circumference = (18.14 cm / 6) * 3.14 * 18.14 cm\n",
            "Circumference = 18.14 cm / 18.14 cm\n",
            "Circumference = 0.08 cm\n",
            "\n",
            "Therefore, the circumference of the circle is 0.08 centimeters.\n",
            "\n",
            "Therefore, the circumference of the\n",
            "\n",
            "Processing Problem 4...\n",
            "Generating answer with Base Model...\n",
            "Base Model Answer: The speed of a train is determined by its weight and its speed. The speed of a train depends on its weight.  For example, a train traveling at a speed of 60 kilometers per hour would be traveling at a speed of 2.5 kilometers per hour.  A train traveling at a speed of 2.5 kilometers per hour would be traveling at a speed of 2.5 kilometers per hour.  The speed of a train is determined by the distance traveled in the train.  The speed of a train is determined by its speed.  The speed of a train is determined by the number of passengers in the train.  The speed of a train is determined by the distance traveled in the train.  The speed of a train is determined by the number of passengers in the train.\n",
            "If a train is traveling at a speed of 1.2 kilometers per hour, how many kilometers can it travel in 2.5 hours?\n",
            "Answer:  The speed of a train is determined by its weight and its speed.  For example, a train traveling at a speed\n",
            "Generating answer with Fine-tuned Model...\n",
            "Fine-tuned Model Answer: The train travels at a speed of 72 kilometers per hour, and it takes 2.5 hours to travel 72 kilometers. To find out how many kilometers it can travel in 2.5 hours, we can use the formula:\n",
            "\n",
            "Distance = Speed × Time\n",
            "\n",
            "Plugging in the given values:\n",
            "\n",
            "72 kilometers/hour = 72 × 2.5 hours\n",
            "\n",
            "Now, we can solve for the distance:\n",
            "\n",
            "72 kilometers = 72 × 2.5\n",
            "72 kilometers = 40 kilometers\n",
            "\n",
            "Therefore, the train can travel 40 kilometers in 2.5 hours. To find out how many kilometers it can travel in 2.5 hours, we can use the same formula:\n",
            "\n",
            "Distance = Speed × Time\n",
            "\n",
            "Plugging in the given values:\n",
            "\n",
            "40 kilometers = 40 × 2.5\n",
            "40 kilometers = 10 kilometers\n",
            "\n",
            "Therefore, the train can travel 10 kilometers in 2.5 hours. To find out how many kilometers it can travel in 2.5 hours, we can use the same formula:\n",
            "\n",
            "Distance = Speed × Time\n",
            "\n",
            "Plugging in the given values:\n",
            "\n",
            "Processing Problem 5...\n",
            "Generating answer with Base Model...\n",
            "Base Model Answer: There was no way to know how many items had been sold. Since we had no way to know what quantity of items the items were, we only knew that the last item was the same as the last one. We thought that it was possible to guess the price of the items in each batch and then determine the total quantity of the items. We thought that it would be much easier to calculate the total quantity of the items in each batch. However, the results were not as easy as we thought. In the end, we decided to give it a try.  It was a very simple task.  The items were purchased from the store and we had to wait for the final batch to arrive.\n",
            "What is your opinion about the value of the items?\n",
            "Answer:  The price of the items is not really a big issue, but I would not say it is a big issue.  When I started to study the value of the items, I found\n",
            "Generating answer with Fine-tuned Model...\n",
            "Fine-tuned Model Answer: On the first day, 1/3 of the total was sold, so the total was:\n",
            "1/3 * 120 = 450\n",
            "\n",
            "On the second day, 40% of the remainder was sold, so the total was:\n",
            "0.40 * 120 = 40% of 450\n",
            "\n",
            "Now, we know that 120 items are left after 120 days, so the remaining items are:\n",
            "450 - 40% = 70\n",
            "\n",
            "So, there were originally 70 items in the batch.\n",
            "\n",
            "The shop initially had a total of 450 goods, but after selling some, they had a total of:\n",
            "450 - 70 = 420\n",
            "\n",
            "Therefore, there were originally 420 items in the batch.\n",
            "\n",
            "If there were originally 420 items in the batch, then the remaining items must be the original number of items minus the number of items that were left:\n",
            "420 - 420 = 15\n",
            "\n",
            "Therefore, there were originally 15 items in the batch.\n",
            "\n",
            "However, we\n",
            "\n",
            "===== Testing Complete. Results saved to ../res\\math_test_results_20250522_003142.txt =====\n"
          ]
        }
      ],
      "source": [
        "# --- Run Test Cases ---\n",
        "print(\"\\n===== Running Tests and Saving Results =====\")\n",
        "\n",
        "test_problems = [\n",
        "    \"If the length of a rectangle is 8 meters and the width is 5 meters, what is its area?\",\n",
        "    \"Ming has 12 apples. He gives 3 to Hong and 2 to Gang. How many apples does Ming have left?\",\n",
        "    \"The radius of a circle is 6 centimeters. What is its circumference? (Take π as 3.14)\",\n",
        "    \"A train travels at a speed of 72 kilometers per hour. How many kilometers can it travel in 2.5 hours?\",\n",
        "    \"A shop has a batch of goods. On the first day, 1/3 of the total was sold. On the second day, 40% of the remainder was sold. If 120 items are left, how many items were there originally in this batch?\"\n",
        "]\n",
        "\n",
        "# Define output file path with a timestamp\n",
        "output_dir = output_dir_widget.value # Get the output directory from the widget\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_filename = f\"math_test_results_{timestamp}.txt\"\n",
        "output_filepath = os.path.join(output_dir, output_filename)\n",
        "\n",
        "with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Math Problem Test Results\\n\")\n",
        "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
        "    f.write(f\"Fine-tuned Model: {model_path_widget.value}\\n\")\n",
        "    f.write(f\"Base Model: {base_model_name_widget.value}\\n\")\n",
        "    f.write(f\"Device: {current_device}\\n\")\n",
        "    f.write(f\"Max Length: {max_length_widget.value}\\n\")\n",
        "    f.write(f\"Temperature: {temperature_widget.value}\\n\")\n",
        "    f.write(f\"Top-p: {top_p_widget.value}\\n\")\n",
        "    f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "    for i, problem in enumerate(test_problems, 1):\n",
        "        print(f\"\\nProcessing Problem {i}...\")\n",
        "        f.write(f\"===== Problem {i} =====\\n\")\n",
        "        f.write(f\"Question: {problem}\\n\\n\")\n",
        "\n",
        "        # Generate answer from base model\n",
        "        if base_model and base_tokenizer:\n",
        "            print(\"Generating answer with Base Model...\")\n",
        "            base_answer = generate_answer(base_model, base_tokenizer, problem)\n",
        "            f.write(f\"Base Model Answer: {base_answer}\\n\\n\")\n",
        "            print(f\"Base Model Answer: {base_answer}\")\n",
        "        else:\n",
        "            f.write(\"Base Model not loaded, skipping generation.\\n\\n\")\n",
        "            print(\"Base Model not loaded, skipping generation.\")\n",
        "\n",
        "        # Generate answer from fine-tuned model\n",
        "        if fine_tuned_model and fine_tuned_tokenizer:\n",
        "            print(\"Generating answer with Fine-tuned Model...\")\n",
        "            fine_tuned_answer = generate_answer(fine_tuned_model, fine_tuned_tokenizer, problem)\n",
        "            f.write(f\"Fine-tuned Model Answer: {fine_tuned_answer}\\n\")\n",
        "            print(f\"Fine-tuned Model Answer: {fine_tuned_answer}\")\n",
        "        else:\n",
        "            f.write(\"Fine-tuned Model not loaded, skipping generation.\\n\")\n",
        "            print(\"Fine-tuned Model not loaded, skipping generation.\")\n",
        "\n",
        "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "print(f\"\\n===== Testing Complete. Results saved to {output_filepath} =====\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a22f7ede-3815-4c5a-ace6-fe62d5e916f8",
      "metadata": {
        "id": "a22f7ede-3815-4c5a-ace6-fe62d5e916f8"
      },
      "source": [
        "### *Part B: Interactive Math Test for Fine-tune GTP-2 Model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e9a0e2-3527-4fd8-8099-e3d429cc44ff",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-21T01:51:33.858415Z",
          "start_time": "2025-05-21T01:51:31.220288Z"
        },
        "execution": {
          "iopub.execute_input": "2025-05-21T02:20:28.784901Z",
          "iopub.status.busy": "2025-05-21T02:20:28.784901Z",
          "iopub.status.idle": "2025-05-21T02:20:28.813263Z",
          "shell.execute_reply": "2025-05-21T02:20:28.813263Z",
          "shell.execute_reply.started": "2025-05-21T02:20:28.784901Z"
        },
        "colab": {
          "referenced_widgets": [
            "930220a0b0ad4dbaa7b478a71fa07018",
            "8727b74deeb24e959e83932464fb6376",
            "7f0f77149c094b429c554125f37e3b59"
          ]
        },
        "id": "a8e9a0e2-3527-4fd8-8099-e3d429cc44ff",
        "outputId": "179eef1a-dbc6-4e85-ef57-cda84d0757a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Interactive Mode (using Fine-tuned Model) =====\n",
            "Enter math problems below. Type 'q' or 'quit' to exit.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "930220a0b0ad4dbaa7b478a71fa07018",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Textarea(value='', description='Problem:', layout=Layout(height='100px', width='80%'), placeholder='Enter a ma…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8727b74deeb24e959e83932464fb6376",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(description='Generate Answer', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f0f77149c094b429c554125f37e3b59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA cache cleared.\n"
          ]
        }
      ],
      "source": [
        "# --- Interactive Mode ---\n",
        "print(\"\\n===== Interactive Mode (using Fine-tuned Model) =====\")\n",
        "print(\"Enter math problems below. Type 'q' or 'quit' to exit.\")\n",
        "\n",
        "# Create a text area for user input in interactive mode\n",
        "interactive_input = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter a math problem here...\",\n",
        "    description=\"Problem:\",\n",
        "    layout=widgets.Layout(width='80%', height='100px')\n",
        ")\n",
        "\n",
        "# Create a button to generate answer\n",
        "generate_button = widgets.Button(description=\"Generate Answer\")\n",
        "\n",
        "# Create an output area for the model's answer\n",
        "interactive_output_area = widgets.Output()\n",
        "\n",
        "def on_generate_button_clicked(b):\n",
        "    with interactive_output_area:\n",
        "        clear_output(wait=True)\n",
        "        user_input = interactive_input.value.strip()\n",
        "        if not user_input:\n",
        "            print(\"Input cannot be empty. Please enter a problem.\")\n",
        "            return\n",
        "\n",
        "        if fine_tuned_model and fine_tuned_tokenizer:\n",
        "            print(\"Generating answer...\")\n",
        "            answer = generate_answer(fine_tuned_model, fine_tuned_tokenizer, user_input)\n",
        "            print(f\"Model Answer: {answer}\")\n",
        "        else:\n",
        "            print(\"Fine-tuned model not available for interactive mode. Please load models first.\")\n",
        "\n",
        "generate_button.on_click(on_generate_button_clicked)\n",
        "\n",
        "# Display interactive widgets\n",
        "display(interactive_input, generate_button, interactive_output_area)\n",
        "\n",
        "# Optional: Clear CUDA cache if using GPU, to free up memory\n",
        "if current_device and current_device.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA cache cleared.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe6f486cc9cf76b",
      "metadata": {
        "id": "3fe6f486cc9cf76b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}